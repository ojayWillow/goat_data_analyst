# ðŸ“– WEEK 3: Narrative Generator

**Goal:** Make results understandable to non-technical users  
**Hours:** 40h | **Tests:** +30 | **Score:** 8.9/10

## Why Week 3?
Without narrative, you have:
- Raw numbers
- Anomalies detected
- Predictions made

But users don't know:
- **"What does this mean?"**
- **"What should I do about it?"**
- **"Where are my problems?"**

Narrative generator answers these questions.

## Architecture

```
agents/narrative_generator/
â”œâ”€â”€ narrative_generator.py      â† Orchestrator
â””â”€â”€ workers/
    â”œâ”€â”€ insight_extractor.py    â† Extract key stats
    â”œâ”€â”€ problem_identifier.py   â† What's wrong?
    â”œâ”€â”€ action_recommender.py   â† What to do?
    â””â”€â”€ story_builder.py        â† Build narrative text
```

## Daily Breakdown

### Day 11 (Fri, Dec 20) - Insight Extractor (8h)
**Focus:** Pull key findings from agent results

**Build:**
- Parse anomaly detector results â†’ extract key anomalies
- Parse predictor results â†’ extract accuracy/confidence
- Parse recommender results â†’ extract top recommendations
- Parse reporter results â†’ extract statistics
- Score importance of each insight

**Code Example:**
```python
class InsightExtractor:
    def extract_anomalies(self, results):
        # Returns: {count, severity, top_anomalies, % of data}
        
    def extract_predictions(self, results):
        # Returns: {accuracy, confidence, top_features, trend}
        
    def extract_recommendations(self, results):
        # Returns: {top_3_actions, confidence, data_issues}
```

**Tests:**
```
âœ“ Extract key anomalies from results
âœ“ Calculate anomaly percentage
âœ“ Extract prediction accuracy
âœ“ Extract feature importance
âœ“ Extract top 3 recommendations
âœ“ Score insight importance
âœ“ Handle missing results gracefully
âœ“ Validation: scores 0-1 scale
```

**Success Criteria:**
- [ ] 8+ tests passing
- [ ] All agent result types parsed
- [ ] Key insights extracted accurately

---

### Day 12 (Sat, Dec 21) - Problem Identifier (8h)
**Focus:** Identify what's wrong with the data

**Build:**
- Classify problems: anomalies, missing data, low predictions, bad distributions
- Rank by severity (critical, high, medium, low)
- Explain impact of each problem
- Suggest which to fix first

**Code Example:**
```python
class ProblemIdentifier:
    def identify_problems(self, results):
        # Returns: [{type, severity, description, impact, location}]
        
        problems = []
        
        # Problem 1: Anomalies
        if results['anomalies']['count'] > threshold:
            problems.append({
                'type': 'anomaly',
                'severity': 'high',
                'count': results['anomalies']['count'],
                'description': f"{count} unusual values detected",
                'impact': 'Skews averages, affects predictions'
            })
        
        # Problem 2: Missing data
        # Problem 3: Poor predictions
        # Problem 4: Outliers
        
        return sorted(problems, key=lambda x: severity_score(x['severity']))
```

**Tests:**
```
âœ“ Detect anomalies as problem
âœ“ Detect missing data as problem
âœ“ Detect low prediction confidence as problem
âœ“ Severity scoring: critical > high > medium > low
âœ“ Multiple problems identified and ranked
âœ“ Impact descriptions are helpful
âœ“ Handle clean datasets (no problems)
âœ“ Validate problem structure
```

**Success Criteria:**
- [ ] 8+ tests passing
- [ ] Problems correctly identified and ranked
- [ ] Impact descriptions are clear

---

### Day 13 (Sun, Dec 22) - Action Recommender (8h)
**Focus:** Tell users what to do

**Build:**
- For each problem, generate actionable recommendations
- Rank by priority and impact
- Explain why the action matters
- Provide next steps

**Code Example:**
```python
class ActionRecommender:
    def recommend_actions(self, problems):
        # Returns: [{priority, action, detail, impact}]
        
        actions = []
        
        for problem in problems:
            if problem['type'] == 'anomaly':
                actions.append({
                    'priority': problem['severity'],
                    'action': 'Investigate anomalies',
                    'detail': f"Found {problem['count']} unusual values. "
                             f"Investigate {problem['location']} first.",
                    'impact': 'Improves model accuracy by ~5%'
                })
            elif problem['type'] == 'missing_data':
                actions.append({
                    'priority': 'high',
                    'action': 'Handle missing data',
                    'detail': f"Fill {problem['%']}% missing values or exclude rows",
                    'impact': 'Improves data completeness from {old}% to {new}%'
                })
        
        return sorted(actions, key=priority_score)
```

**Tests:**
```
âœ“ Anomaly problem â†’ investigation action
âœ“ Missing data problem â†’ handling action
âœ“ Low prediction problem â†’ data improvement action
âœ“ Actions are prioritized correctly
âœ“ Actions are specific (not generic)
âœ“ Impact is quantified when possible
âœ“ Multiple actions for multiple problems
âœ“ Handle edge cases (no problems)
```

**Success Criteria:**
- [ ] 8+ tests passing
- [ ] Actions are specific and actionable
- [ ] Priorities are correct

---

### Day 14 (Mon, Dec 23) - Story Builder (8h)
**Focus:** Combine insights into readable narrative

**Build:**
- Combine insights + problems + actions into narrative
- Write in plain English (non-technical)
- Structure: headline â†’ summary â†’ problems â†’ actions â†’ next steps
- Output as JSON object

**Code Example:**
```python
class StoryBuilder:
    def build_narrative(self, insights, problems, actions):
        # Returns: {headline, summary, problems, actions, next_steps}
        
        return {
            'headline': f"Your data shows {insights['anomaly_count']} "
                       f"anomalies and {insights['prediction_accuracy']}% "
                       f"prediction confidence",
            
            'summary': f"Your dataset contains {insights['rows']} records. "
                      f"We found {len(problems)} issues and {len(actions)} "
                      f"recommendations to improve data quality.",
            
            'problems': problems,
            'actions': actions,
            
            'next_steps': [
                f"1. {actions[0]['action'].lower()}",
                f"2. {actions[1]['action'].lower() if len(actions) > 1 else '...'}"
            ],
            
            'confidence': self._calculate_narrative_confidence(insights)
        }
```

**Output Example:**
```json
{
  "headline": "Your data shows 23 anomalies and 87% prediction confidence",
  "summary": "Your dataset contains 10,000 records. We found 3 issues and 5 recommendations to improve data quality.",
  "problems": [
    {
      "type": "anomaly",
      "severity": "high",
      "description": "23 transactions exceed normal patterns by 5x",
      "location": "Region: North, Dec 10",
      "impact": "Skews average sales by 12%"
    }
  ],
  "actions": [
    {
      "priority": "high",
      "action": "Investigate anomalies",
      "detail": "23 unusual values found. Focus on North region Dec 10.",
      "impact": "Improves accuracy by ~5%"
    }
  ],
  "next_steps": [
    "1. Investigate the North region spike on Dec 10",
    "2. Fill missing Q1 data or exclude from training",
    "3. Consider holiday seasonality in your forecasts"
  ]
}
```

**Tests:**
```
âœ“ Narrative includes headline
âœ“ Narrative includes summary
âœ“ Narrative includes problems
âœ“ Narrative includes actions
âœ“ Narrative includes next steps
âœ“ Headline is engaging but accurate
âœ“ Summary mentions key numbers
âœ“ Actions are in priority order
âœ“ Language is non-technical
âœ“ JSON structure is valid
```

**Success Criteria:**
- [ ] 8+ tests passing
- [ ] Narratives are clear and helpful
- [ ] Structure is consistent

---

### Day 15 (Tue, Dec 24) - Integration + Testing (8h)
**Focus:** Full narrative pipeline working

**Build:**
- Integrate narrative generator into orchestrator
- Narrative is part of /analyze response
- Test with real data scenarios
- Validate narratives are accurate

**Tests:**
```
âœ“ Orchestrator â†’ Narrative Generator â†’ Combined result
âœ“ Dataset with anomalies: narrative identifies them
âœ“ Dataset with missing data: narrative identifies it
âœ“ Dataset all clean: narrative says "data looks good"
âœ“ Narrative for quick pipeline vs full pipeline
âœ“ Performance: narrative generation < 1s
âœ“ Narrative survives large datasets (100K rows)
âœ“ Multiple scenarios tested (10+ datasets)
```

**Success Criteria:**
- [ ] 8+ tests passing
- [ ] Narrative generator integrated
- [ ] Narratives accurate on test data
- [ ] Performance maintained

---

### Week 3 Exit Criteria âœ…
- âœ… Narrative generator fully functional
- âœ… 30+ new tests (insight, problem, action, story, integration)
- âœ… 214 total tests passing (184 + 30)
- âœ… Users get clear guidance on their data
- âœ… **Score: 8.9/10** (User-Friendly)
