# ðŸ§ª PHASE 2 TEST SCENARIOS

**Purpose:** Define what we're testing in Week 2  
**Status:** Ready for execution  
**Date:** December 10, 2025

---

## SCENARIO 1: Happy Path (Normal Operation)

**What:** Test all 8 agents with clean, normal data

**Input Data:**
- File: `tests/data/small_dataset.csv`
- Rows: 2,000
- Columns: 10 (mixed numeric, categorical, dates)
- Quality: Clean (no missing values, no outliers)
- Size: <1MB

**What Each Agent Tests:**

### DataLoader
```
Task: Load CSV file
Input: tests/data/small_dataset.csv
Expected Output:
  - pandas DataFrame loaded
  - Shape: (2000, 10)
  - All columns readable
  - No exceptions
  
Success Criteria:
  âœ“ Returns DataFrame
  âœ“ No errors
  âœ“ Structured log shows load event
  âœ“ Completes in <2 seconds
```

### Explorer
```
Task: Analyze data structure and statistics
Input: DataFrame from DataLoader
Expected Output:
  - Column statistics (mean, median, std, etc)
  - Data types identified
  - Missing value report
  - Basic correlations
  
Success Criteria:
  âœ“ Returns analysis dict
  âœ“ Contains expected keys
  âœ“ Structured logging captured
  âœ“ Completes in <5 seconds
```

### Aggregator
```
Task: Group and aggregate data
Input: DataFrame, groupby column='category_1', agg_col='value_1', agg='mean'
Expected Output:
  - Aggregated results grouped by category
  - Summary statistics
  
Success Criteria:
  âœ“ Returns aggregated DataFrame
  âœ“ Groups created correctly
  âœ“ Aggregation completed
  âœ“ Completes in <5 seconds
```

### Predictor
```
Task: Make predictions on data
Input: DataFrame with features
Expected Output:
  - Prediction values
  - Model information
  
Success Criteria:
  âœ“ Returns predictions
  âœ“ No errors
  âœ“ Predictions are numeric
  âœ“ Completes in <10 seconds
```

### AnomalyDetector
```
Task: Detect anomalies in data
Input: DataFrame
Expected Output:
  - Anomaly flags (0 or 1)
  - Anomaly scores
  - Threshold used
  
Success Criteria:
  âœ“ Returns anomaly results
  âœ“ No exceptions
  âœ“ Results make sense
  âœ“ Completes in <10 seconds
```

### Recommender
```
Task: Generate recommendations based on analysis
Input: Analysis results from Explorer
Expected Output:
  - List of actionable recommendations
  - Priority/severity
  
Success Criteria:
  âœ“ Returns recommendation list
  âœ“ Recommendations are meaningful
  âœ“ Logged correctly
  âœ“ Completes in <5 seconds
```

### Reporter
```
Task: Generate report from analysis
Input: All analysis results
Expected Output:
  - Report dict (JSON-serializable)
  - Executive summary
  - Detailed findings
  
Success Criteria:
  âœ“ Returns report dict
  âœ“ Report contains expected sections
  âœ“ JSON-serializable
  âœ“ Completes in <5 seconds
```

### Visualizer
```
Task: Create visualizations
Input: DataFrame
Expected Output:
  - Plot files created
  - Chart objects
  
Success Criteria:
  âœ“ Charts generated
  âœ“ Files saved
  âœ“ No rendering errors
  âœ“ Completes in <10 seconds
```

---

## SCENARIO 2: Edge Cases (Data Quality Issues)

**What:** Test agents with real-world messy data

**Input Data:**
- File: `tests/data/medium_dataset.csv`
- Rows: 100,000
- Columns: 15 (mixed types)
- Quality Issues:
  - Missing values (10-20% of some columns)
  - Duplicate rows (1-2%)
  - Outliers (1% extreme values)
  - Mixed data types (some inconsistencies)

**What We're Testing:**

### Agents Handle Missing Data
```
Test: Does agent handle NaN values gracefully?
Expected:
  âœ“ No crashes
  âœ“ Error logged if column critical
  âœ“ Reasonable handling (skip, interpolate, or remove)
```

### Agents Handle Duplicates
```
Test: Does agent handle duplicate rows?
Expected:
  âœ“ Identifies duplicates
  âœ“ Handles appropriately
  âœ“ Logged in structured logs
```

### Agents Handle Outliers
```
Test: Does agent handle extreme values?
Expected:
  âœ“ Identifies outliers
  âœ“ Doesn't crash
  âœ“ May flag as anomalies
```

### Agents Handle Scale Differences
```
Test: Columns with different scales/ranges?
Expected:
  âœ“ Normalizes if needed
  âœ“ Results reasonable
  âœ“ Properly logged
```

### Agents Handle Empty Groups
```
Test: What if groupby creates empty groups?
Expected:
  âœ“ Handles gracefully
  âœ“ Skips or marks as 0/null
  âœ“ No crashes
```

**Success Criteria for Scenario 2:**
```
âœ“ All agents complete without unhandled exceptions
âœ“ Errors logged clearly
âœ“ Results are reasonable despite data quality issues
âœ“ Agents handle missing values appropriately
âœ“ No silent failures (all issues logged)
âœ“ Completes in <60 seconds total for all agents
```

---

## SCENARIO 3: Stress Test (Large Data)

**What:** Test agents with large dataset to check performance

**Input Data:**
- File: `tests/data/medium_dataset.csv` (same as Scenario 2)
- Rows: 100,000
- Columns: 15
- Total Size: ~20-30MB (loaded in memory)

**What We're Testing:**

### Performance Under Load
```
Test: Do agents complete in reasonable time?
Expected:
  âœ“ DataLoader: <5 seconds
  âœ“ Explorer: <10 seconds
  âœ“ Aggregator: <10 seconds
  âœ“ Predictor: <20 seconds
  âœ“ AnomalyDetector: <20 seconds
  âœ“ Reporter: <10 seconds
  âœ“ Visualizer: <15 seconds
```

### Memory Usage
```
Test: Memory stays reasonable?
Expected:
  âœ“ No memory leaks (peak reasonable)
  âœ“ Memory released after operation
  âœ“ No excessive swapping
```

### CPU Usage
```
Test: CPU usage reasonable?
Expected:
  âœ“ Operations complete efficiently
  âœ“ No hanging/blocking
  âœ“ All cores utilized appropriately
```

### Logging Overhead
```
Test: Logging doesn't slow things down significantly?
Expected:
  âœ“ <5% performance overhead
  âœ“ Logging doesn't block operations
  âœ“ Structured logs captured
```

**Success Criteria for Scenario 3:**
```
âœ“ All agents complete under 2 minutes total
âœ“ Individual agents meet performance targets
âœ“ Memory usage stays under 2GB
âœ“ No timeouts or hangs
âœ“ Logging captures all operations
```

---

## TEST EXECUTION ORDER

### Day 1 (Dec 11) Morning
1. Create test data files
2. Verify test data created correctly
3. Run Scenario 1 (Happy Path)
   - Quick smoke test of all agents
   - Verify basic functionality
   - Check structured logging works

### Day 1 Afternoon
4. Run Scenario 2 (Edge Cases)
   - Test error handling
   - Verify graceful degradation
   - Check error messages clear

### Day 2 (Dec 12) Morning
5. Run Scenario 3 (Stress Test)
   - Measure performance
   - Capture benchmarks
   - Identify bottlenecks

---

## PASS/FAIL CRITERIA

### Scenario 1 (Happy Path) - MUST PASS
```
âœ“ All 8 agents execute successfully
âœ“ No unhandled exceptions
âœ“ Results are meaningful
âœ“ Structured logging works

If FAILS: Block further testing until fixed
```

### Scenario 2 (Edge Cases) - SHOULD PASS
```
âœ“ Agents handle data quality issues
âœ“ Clear error messages logged
âœ“ Graceful handling of problems

If FAILS: Document as limitation, continue testing
```

### Scenario 3 (Stress Test) - SHOULD PASS
```
âœ“ Performance meets targets
âœ“ No memory leaks
âœ“ Agents scale to 100K rows

If FAILS: Document performance limits, plan optimization
```

---

## TEST DATA LOCATIONS

```
tests/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ small_dataset.csv (2,000 rows, clean)
â”‚   â”œâ”€â”€ medium_dataset.csv (100,000 rows, with issues)
â”‚   â”œâ”€â”€ test_data.json (5,000 records)
â”‚   â””â”€â”€ test_data.xlsx (5,000 rows)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ [test results will be saved here]
â”œâ”€â”€ PHASE2_TEST_SCENARIOS.md (this file)
â”œâ”€â”€ PHASE2_SUCCESS_CRITERIA.md (detailed criteria)
â””â”€â”€ run_phase2_tests.py (test runner script)
```

---

**Next Step:** Create PHASE2_SUCCESS_CRITERIA.md with detailed criteria for each agent.
